--- # tasks file for spark_proj

      ####################### Update /etc/hosts file     ##################

    - include: etchost.yaml
      ####################### Update /etc/hosts file     ##################
    
    - name: Create service account for Spark
      user: name={{ spark_user }}
            system=yes
            home={{ spark_lib_dir }}
            shell={{ spark_user_shell }}
            state=present
            groups="{{ spark_user_groups | join(',')}}"
      tags: ["spark-user"]

    
      ####################### Spark user passwordless SSH ##################
    - name: SSH KeyGen command
      become: yes
      become_user: spark
      tags: run
      shell: 
        ssh-keygen -q -b 2048 -t rsa -N "" -C "creating SSH" -f ~/.ssh/id_rsa
        creates="~/.ssh/id_rsa"

    - name: Fetch the keyfile from one server to another
      become: yes
      become_user: spark
      tags: run
      fetch:
        src: "~/.ssh/id_rsa.pub"
        dest: "buffer/{{ansible_hostname}}-id_rsa.pub"
        flat: yes

    - name: Copy the key add to authorized_keys using Ansible module
      become: yes
      become_user: spark
      tags: run
      authorized_key:
        user: spark
        state: present
        key: "{{ lookup('file','buffer/Master-1-id_rsa.pub')}}"
      when: ansible_hostname != "{{ item }}" or ansible_hostname == "{{ item }}"
      with_items: "{{groups['tag_cluster_spk']}}"


 #    when: "{{ item.dest != ansible_hostname }}"
 #    with_items:
 #      - { dest: "{{groups['tag_cluster_spk'][2]}}"}
 #      - { dest: "{{groups['tag_cluster_spk'][1]}}"}
 #      - { dest: "{{groups['tag_cluster_spk'][0]}}"}
          #when: "{{ item != ansible_hostname }}"
          #with_items:
          #- "{{ groups['demo']}}"

          #########################################################      
    - name: Ensure Spark configuration directory exists
      file: path="{{ spark_conf_dir}}"
            state=directory
      tags: ["config"]

    - name: Ensure Spark log and run directories exist
      file: path="{{ item}}"
            owner={{spark_user}}
            group={{ spark_user }}
            mode=0775
            state=directory
      with_items:
        - "{{spark_log_dir}}"
        - "{{spark_run_dir}}"

    - name: Ensure Spark download and install directories exist
      file: path="{{ item}}"
            mode=0755
            state=directory
            follow=true
      with_items:
        - "{{spark_src_dir}}"
        - "{{spark_usr_parent_dir}}"

    - name: Download Spark distribution
      get_url: url="{{spark_mirror}}/spark-{{spark_version}}.tgz"
               dest="{{spark_src_dir}}/spark-{{spark_version}}.tgz"
    
    - name: Extract Spark distribution
      unarchive: src="{{spark_src_dir}}/spark-{{spark_version}}.tgz"
                 dest="{{spark_usr_parent_dir}}"
                 copy=no
                 creates="{{spark_usr_parent_dir}}/spark-{{spark_version}}"
                 owner={{spark_user}}
                 group={{ spark_user }}
    
    - name: Setup Spark distribution symlinks
      file: src="{{item.src}}"
            dest="{{item.dest}}"
            state=link
      with_items:
        - { src: "/usr/lib/spark-{{spark_version}}", dest: "/usr/lib/spark"}
        - { src: "/usr/lib/spark-{{spark_version}}/conf", dest: "/etc/spark/conf"}
      when: spark_symlinks_enabled
      tags: ["symlinks"]
      
      
    - name: Create shims for Spark binaries
      template: src=spark-shim.j2
                dest="/usr/bin/{{item}}"
                mode=0755
      with_items:
        - spark-class
        - spark-shell
        - spark-sql
        - spark-submit
      when: spark_shims_enabled
      tags: ["shims"]

    - name: Ensure Spark work and temp work directories exist
     # Execute this task after setting up the symlinks to allow directories below them
      file: path="{{item}}"
        owner={{spark_user}}
        group={{spark_user}}
        mode=0775
        state=directory
      with_items:
        - "{{spark_work_dir}}"
        - "{{spark_tmp_dir}}"

    - name: Ensure Spark local directories exist
      file: path="{{item}}"
            owner={{spark_user}}
            group={{spark_user}}
            mode={{spark_local_dir_mode}}
            state=directory
      when: item != "/tmp"
      with_items: "{{spark_local_dirs}}"
      tags: ["spark-local-dirs"]

    - name: Configure Spark environment
      template: src=spark-env.sh.j2
                dest="{{ spark_usr_parent_dir }}/spark-{{ spark_version }}/conf/spark-env.sh"
      tags: ["config"]

    - name: Configure Spark defaults config file
      template: src=spark-defaults.conf.j2
                dest="{{ spark_usr_parent_dir }}/spark-{{ spark_version }}/conf/spark-defaults.conf"
      tags: ["config"]
  
      #     - include: spark-hive-site.yml
      #  tags: ["config","spark-hive-site"]

    - include: spark-log4j.yml
      vars:
        log4j_template_file: "{{spark_log4j_template_file | default('log4j.properties')}}"
      tags: ["config","spark-log4j"]

    - name: add spark profile to startup
      template:
        src: spark-profile.sh.j2
        dest: /etc/profile.d/spark-profile.sh
        mode: 0644

    - name: set slaves
      template: src="slaves.j2" dest="{{spark_usr_parent_dir}}/spark-{{spark_version}}/conf/slaves"    

    - name: start spark master service
      become: true
      become_user: '{{spark_user}}'
      shell: "sbin/start-all.sh"
      args:
        chdir: "{{spark_usr_parent_dir}}/spark/"
      ignore_errors: no
      when: inventory_hostname in groups['tag_Env_Master']
           
    - name: create spark event log dir
      file: path="{{spark_eventlog_dir}}"
            owner={{spark_user}}
            group={{ spark_user }}
            state=directory
            mode=0775

    - name: start spark history server service
      become: true
      become_user: '{{spark_user}}'
      shell: "sbin/start-history-server.sh"
      args:
        chdir: "{{spark_usr_parent_dir}}/spark/"
      ignore_errors: yes
      when: inventory_hostname in groups['tag_Env_Master']













      
